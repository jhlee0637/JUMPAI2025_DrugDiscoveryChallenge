# -*- coding: utf-8 -*-
"""
ASK1 IC50 ì˜ˆì¸¡ ê³ ì„±ëŠ¥ íŒŒì´í”„ë¼ì¸ (ì™„ì „ ìˆ˜ì • ë²„ì „)
- CAS ë°ì´í„° ë¡œë”© ë¬¸ì œ í•´ê²°
- Mordred ì˜¤ë¥˜ í•´ê²° (RDKit ì „ìš©)
- scikit-learn ì™„ì „ í˜¸í™˜
- Apple Silicon ìµœì í™”
- ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íŠ¹ì„± ì¶”ì¶œ
- ê°œì„ ëœ ì•™ìƒë¸” ì „ëµ
"""

import os
import joblib
import warnings
import numpy as np
import pandas as pd
import gc
from rdkit import Chem
from rdkit.Chem import rdFingerprintGenerator, Descriptors
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import cross_val_score, train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import lightgbm as lgb
import xgboost as xgb
import catboost as c
import optuna
import shap
from typing import Dict, List, Tuple, Optional
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import psutil

# Mordred ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - RDKitë§Œ ì‚¬ìš©
MORDRED_AVAILABLE = False
print("â„¹ï¸ RDKit ì „ìš© ëª¨ë“œ - Mordred ì˜ì¡´ì„± ì œê±°")

warnings.filterwarnings("ignore")
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Apple Silicon ìµœì í™” ì„¤ì •
os.environ['OMP_NUM_THREADS'] = str(max(1, psutil.cpu_count() - 1))
os.environ['OMP_WAIT_POLICY'] = 'active'
os.environ['LIGHTGBM_EXEC'] = 'lightgbm'

# ì „ì—­ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
SEED = 42
N_FOLD = 5  # 10 -> 5ë¡œ ì¶•ì†Œ
N_BITS = 1024  # 2048 -> 1024ë¡œ ì¶•ì†Œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
N_TRIALS = 30  # 100 -> 30ìœ¼ë¡œ ì¶•ì†Œ
N_JOBS = max(1, psutil.cpu_count() - 1)  # CPU ì½”ì–´ ìˆ˜ ìë™ ì„¤ì •

# ë°ì´í„° ê²½ë¡œ
DATA_PATHS = {
    "cas": "/Users/skku_aws28/Documents/Jump_Team_Project/Data/CAS_KPBMA_MAP3K5_IC50s.xlsx",
    "chembl": "/Users/skku_aws28/Documents/Jump_Team_Project/Data/ChEMBL_ASK1(IC50).csv",
    "pubchem": "/Users/skku_aws28/Documents/Jump_Team_Project/Data/PubChem_ASK1.csv",
    "test": "/Users/skku_aws28/Documents/Jump_Team_Project/Data/test.csv",
    "sample": "/Users/skku_aws28/Documents/Jump_Team_Project/Data/sample_submission.csv"
}

class OptimizedMolecularFeaturizer:
    """RDKit ì „ìš© ë¶„ì íŠ¹ì„± ì¶”ì¶œê¸° (Mordred ì˜¤ë¥˜ í•´ê²°)"""
    
    def __init__(self):
        self.morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=N_BITS)
        self.feature_cache = {}
        print("âœ… RDKit ê¸°ë°˜ ë¶„ì íŠ¹ì„± ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
    def extract_morgan_features(self, mol) -> np.ndarray:
        """Morgan Fingerprint ì¶”ì¶œ"""
        if mol is None:
            return np.zeros(N_BITS, dtype=np.float32)
        return self.morgan_gen.GetFingerprintAsNumPy(mol).astype(np.float32)
    
    def extract_rdkit_features(self, mol) -> np.ndarray:
        """í™•ì¥ëœ RDKit Descriptor ì¶”ì¶œ (Mordred ëŒ€ì²´)"""
        if mol is None:
            return np.zeros(30, dtype=np.float32)
        
        # í¬ê´„ì ì¸ RDKit descriptor ë¦¬ìŠ¤íŠ¸ (Mordred ê¸°ëŠ¥ ëŒ€ì²´)
        descriptors_list = [
            # ê¸°ë³¸ ë¬¼ë¦¬í™”í•™ì  íŠ¹ì„±
            Descriptors.MolWt, Descriptors.MolLogP, Descriptors.NumHDonors,
            Descriptors.NumHAcceptors, Descriptors.TPSA, Descriptors.NumRotatableBonds,
            
            # êµ¬ì¡°ì  íŠ¹ì„±
            Descriptors.NumAromaticRings, Descriptors.NumAliphaticRings,
            Descriptors.NumSaturatedRings, Descriptors.RingCount,
            Descriptors.HeavyAtomCount, Descriptors.NumHeteroatoms,
            
            # ë³µì¡ì„± ì§€í‘œ
            Descriptors.BertzCT, Descriptors.BalabanJ, Descriptors.HallKierAlpha,
            
            # ì—°ê²°ì„± ì§€í‘œ
            Descriptors.Kappa1, Descriptors.Kappa2, Descriptors.Kappa3,
            Descriptors.Chi0v, Descriptors.Chi1v, Descriptors.Chi2v,
            
            # ì¶”ê°€ íŠ¹ì„±
            Descriptors.FractionCSP3, Descriptors.MolMR, Descriptors.LabuteASA,
            Descriptors.MaxEStateIndex, Descriptors.MinEStateIndex,
            Descriptors.MaxAbsEStateIndex, Descriptors.MaxPartialCharge,
            Descriptors.MinPartialCharge, Descriptors.NumRadicalElectrons
        ]
        
        features = []
        for desc_func in descriptors_list:
            try:
                value = desc_func(mol)
                features.append(0.0 if pd.isna(value) or np.isinf(value) else float(value))
            except:
                features.append(0.0)
        
        # ì •í™•íˆ 30ê°œ ë°˜í™˜
        return np.array(features[:30], dtype=np.float32)
    
    def extract_additional_features(self, mol) -> np.ndarray:
        """ì¶”ê°€ ë¶„ì íŠ¹ì„± (Mordred ëŒ€ì²´)"""
        if mol is None:
            return np.zeros(10, dtype=np.float32)
        
        try:
            # ì¶”ê°€ ê³„ì‚° ê°€ëŠ¥í•œ íŠ¹ì„±ë“¤
            features = []
            
            # ì›ì ê°œìˆ˜ ê´€ë ¨
            features.append(float(mol.GetNumAtoms()))
            features.append(float(mol.GetNumBonds()))
            
            # ë°©í–¥ì¡±ì„± ê´€ë ¨
            aromatic_atoms = sum(1 for atom in mol.GetAtoms() if atom.GetIsAromatic())
            features.append(float(aromatic_atoms))
            
            # í•˜ì´ë¸Œë¦¬ë“œí™” ê´€ë ¨
            sp3_atoms = sum(1 for atom in mol.GetAtoms() if atom.GetHybridization() == Chem.HybridizationType.SP3)
            features.append(float(sp3_atoms))
            
            # ì „í•˜ ê´€ë ¨
            formal_charge = sum(atom.GetFormalCharge() for atom in mol.GetAtoms())
            features.append(float(formal_charge))
            
            # ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì±„ì›€
            while len(features) < 10:
                features.append(0.0)
            
            return np.array(features[:10], dtype=np.float32)
        except:
            return np.zeros(10, dtype=np.float32)
    
    def featurize(self, smiles: str) -> np.ndarray:
        """í†µí•© ë¶„ì íŠ¹ì„± ì¶”ì¶œ"""
        if smiles in self.feature_cache:
            return self.feature_cache[smiles]
        
        mol = Chem.MolFromSmiles(smiles)
        
        # Morgan Fingerprint
        morgan_fp = self.extract_morgan_features(mol)
        
        # RDKit Descriptors
        rdkit_desc = self.extract_rdkit_features(mol)
        
        # ì¶”ê°€ íŠ¹ì„±
        additional_desc = self.extract_additional_features(mol)
        
        # íŠ¹ì„± ê²°í•© (ì´ 1024 + 30 + 10 = 1064 ì°¨ì›)
        combined_features = np.concatenate([morgan_fp, rdkit_desc, additional_desc])
        
        # ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬ ì œí•œ)
        if len(self.feature_cache) < 10000:
            self.feature_cache[smiles] = combined_features
        
        return combined_features

class FastDataLoader:
    """ë¹ ë¥¸ ë°ì´í„° ë¡œë” (CAS ë°ì´í„° ë¡œë”© ë¬¸ì œ í•´ê²°)"""
    
    @staticmethod
    def load_cas_data_properly():
        """CAS ë°ì´í„° ê°œì„ ëœ ë¡œë“œ (ì‹œíŠ¸ë³„ í™•ì¸ ë° í—¤ë” ì²˜ë¦¬)"""
        try:
            print("ğŸ“Š CAS ë°ì´í„° ë¡œë”© ì¤‘...")
            
            excel_file = pd.ExcelFile(DATA_PATHS["cas"])
            sheet_names = excel_file.sheet_names
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œíŠ¸: {sheet_names}")
            
            # ìš°ì„ ìˆœìœ„ ì‹œíŠ¸ ëª©ë¡
            priority_sheets = [
                'MAP3K5 Ligand IC50s',
                'Ligand Number Names SMILES', 
                'Data Dictionary'
            ]
            
            # ëª¨ë“  ì‹œíŠ¸ í™•ì¸
            all_sheets = priority_sheets + [s for s in sheet_names if s not in priority_sheets]
            
            for sheet_name in all_sheets:
                if sheet_name not in sheet_names:
                    continue
                    
                try:
                    print(f"\nğŸ” ì‹œíŠ¸ '{sheet_name}' ë¶„ì„ ì¤‘...")
                    
                    # ì—¬ëŸ¬ skiprows ì‹œë„ (í—¤ë” ë¬¸ì œ í•´ê²°)
                    for skip in [0, 1, 2, 3, 4]:
                        try:
                            # ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¨¼ì € í™•ì¸
                            sample_df = pd.read_excel(DATA_PATHS["cas"], 
                                                    sheet_name=sheet_name, 
                                                    skiprows=skip,
                                                    nrows=10)
                            
                            print(f"  skiprows={skip}: {sample_df.shape}")
                            print(f"  ì»¬ëŸ¼: {list(sample_df.columns)}")
                            
                            # ìœ íš¨í•œ ì»¬ëŸ¼ í™•ì¸
                            valid_cols = [col for col in sample_df.columns 
                                        if not col.startswith('Unnamed') 
                                        and 'Copyright' not in str(col)
                                        and len(str(col).strip()) > 0]
                            
                            if len(valid_cols) >= 2:
                                # SMILESì™€ IC50 ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
                                has_smiles = any('SMILES' in str(col).upper() for col in valid_cols)
                                has_ic50 = any(keyword in str(col).upper() 
                                             for col in valid_cols 
                                             for keyword in ['IC50', 'ACTIVITY', 'VALUE', 'POTENCY', 'CONC'])
                                
                                print(f"  ìœ íš¨ ì»¬ëŸ¼ ìˆ˜: {len(valid_cols)}")
                                print(f"  SMILES ì»¬ëŸ¼: {has_smiles}")
                                print(f"  IC50 ì»¬ëŸ¼: {has_ic50}")
                                
                                if has_smiles or has_ic50 or len(valid_cols) >= 3:
                                    print(f"  âœ… ìœ ë§í•œ ë°ì´í„° ë°œê²¬!")
                                    
                                    # ì „ì²´ ë°ì´í„° ë¡œë“œ
                                    full_df = pd.read_excel(DATA_PATHS["cas"], 
                                                          sheet_name=sheet_name, 
                                                          skiprows=skip,
                                                          nrows=1000)
                                    
                                    print(f"  ì „ì²´ ë°ì´í„° ë¡œë“œ: {full_df.shape}")
                                    return full_df
                                    
                        except Exception as e:
                            print(f"  skiprows={skip} ì‹¤íŒ¨: {str(e)[:50]}...")
                            continue
                            
                except Exception as e:
                    print(f"  ì‹œíŠ¸ '{sheet_name}' ì „ì²´ ì‹¤íŒ¨: {e}")
                    continue
            
            print("âš ï¸ ëª¨ë“  ì‹œíŠ¸ì—ì„œ ì ì ˆí•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
            
        except Exception as e:
            print(f"âŒ CAS ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    @staticmethod
    def load_and_process_all_data():
        """ëª¨ë“  ë°ì´í„° ë¡œë“œ ë° í†µí•© ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        print("ğŸ“ ë¹ ë¥¸ ë°ì´í„° ë¡œë”© ì‹œì‘...\n")
        
        processed_dfs = []
        
        # CAS ë°ì´í„° (ê°œì„ ëœ ë¡œë”)
        cas = FastDataLoader.load_cas_data_properly()
        if not cas.empty:
            cas_processed = FastDataLoader.process_dataset_fast(cas, "CAS")
            if not cas_processed.empty:
                processed_dfs.append(cas_processed)
                print(f"âœ… CAS ë°ì´í„° ì¶”ê°€ë¨: {cas_processed.shape}")
        
        # ChEMBL ë°ì´í„° (ìƒ˜í”Œë§)
        try:
            print("\nğŸ“Š ChEMBL ë°ì´í„° ë¡œë”© ì¤‘...")
            chembl = pd.read_csv(DATA_PATHS["chembl"], sep=';', nrows=2000)
            chembl_processed = FastDataLoader.process_dataset_fast(chembl, "ChEMBL")
            if not chembl_processed.empty:
                processed_dfs.append(chembl_processed)
            print(f"âœ… ChEMBL ë°ì´í„°: {chembl_processed.shape}")
        except Exception as e:
            print(f"âš ï¸ ChEMBL ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
        # PubChem ë°ì´í„° (ìƒ˜í”Œë§)
        try:
            print("\nğŸ“Š PubChem ë°ì´í„° ë¡œë”© ì¤‘...")
            pubchem = pd.read_csv(DATA_PATHS["pubchem"], nrows=2000)
            pubchem_processed = FastDataLoader.process_dataset_fast(pubchem, "PubChem")
            if not pubchem_processed.empty:
                processed_dfs.append(pubchem_processed)
            print(f"âœ… PubChem ë°ì´í„°: {pubchem_processed.shape}")
        except Exception as e:
            print(f"âš ï¸ PubChem ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        try:
            test = pd.read_csv(DATA_PATHS["test"])
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test.shape}")
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            test = pd.DataFrame()
        
        if processed_dfs:
            train_data = pd.concat(processed_dfs, ignore_index=True)
            train_data = train_data.drop_duplicates(subset=['Smiles']).reset_index(drop=True)
            
            # ë°ì´í„° í¬ê¸° ì œí•œ (ì„±ëŠ¥ ìµœì í™”)
            if len(train_data) > 5000:
                train_data = train_data.sample(5000, random_state=SEED).reset_index(drop=True)
            
            print(f"\nğŸ¯ ìµœì¢… í†µí•© ë°ì´í„°: {train_data.shape}")
            if 'source' in train_data.columns:
                print(f"ì†ŒìŠ¤ë³„ ë¶„í¬:\n{train_data['source'].value_counts()}")
            return train_data, test
        else:
            raise ValueError("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    @staticmethod
    def process_dataset_fast(df: pd.DataFrame, source_name: str) -> pd.DataFrame:
        """ë¹ ë¥¸ ë°ì´í„°ì…‹ ì²˜ë¦¬ (ê°œì„ ëœ ì»¬ëŸ¼ ì°¾ê¸°)"""
        df_copy = df.copy()
        df_copy['source'] = source_name
        
        print(f"\nğŸ” {source_name} ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        print(f"  ì›ë³¸ í¬ê¸°: {df_copy.shape}")
        print(f"  ì»¬ëŸ¼ ëª©ë¡: {list(df_copy.columns)}")
        
        # ì»¬ëŸ¼ ì°¾ê¸° (ë” ìœ ì—°í•œ ë°©ì‹)
        smiles_col = None
        ic50_col = None
        
        # SMILES ì»¬ëŸ¼ ì°¾ê¸°
        smiles_keywords = ['SMILES', 'SMILE', 'CANONICAL_SMILES', 'STRUCTURE']
        for col in df_copy.columns:
            col_upper = str(col).upper()
            if any(keyword in col_upper for keyword in smiles_keywords):
                smiles_col = col
                break
        
        # IC50 ì»¬ëŸ¼ ì°¾ê¸° (ê°œì„ ëœ ë°©ì‹)
        ic50_keywords = ['IC50', 'ACTIVITY_VALUE', 'ACTIVITY', 'VALUE', 'POTENCY', 'CONC', 'MEASUREMENT', 'RESULT']
        for col in df_copy.columns:
            col_upper = str(col).upper()
            if any(keyword in col_upper for keyword in ic50_keywords):
                # ìˆ«ì ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                try:
                    sample_values = pd.to_numeric(df_copy[col].dropna().head(10), errors='coerce')
                    if not sample_values.isna().all():
                        ic50_col = col
                        break
                except:
                    continue
        
        # PubChem ë°ì´í„° íŠ¹ë³„ ì²˜ë¦¬
        if source_name == 'PubChem' and ic50_col is None:
            if 'Activity_Value' in df_copy.columns:
                ic50_col = 'Activity_Value'
            elif 'Activity' in df_copy.columns:
                ic50_col = 'Activity'
        
        print(f"  SMILES ì»¬ëŸ¼: {smiles_col}")
        print(f"  IC50 ì»¬ëŸ¼: {ic50_col}")
        
        if smiles_col and ic50_col:
            result = df_copy[[smiles_col, ic50_col, 'source']].copy()
            result.columns = ['Smiles', 'IC50_nM', 'source']
            
            # ë°ì´í„° ì •ë¦¬
            print(f"  ì •ë¦¬ ì „: {result.shape}")
            result = result.dropna()
            print(f"  NA ì œê±° í›„: {result.shape}")
            
            result['IC50_nM'] = pd.to_numeric(result['IC50_nM'], errors='coerce')
            result = result.dropna()
            print(f"  ìˆ«ì ë³€í™˜ í›„: {result.shape}")
            
            result = result[result['IC50_nM'] > 0]
            print(f"  ì–‘ìˆ˜ í•„í„° í›„: {result.shape}")
            
            # ë‹¨ìœ„ ë³€í™˜ ì²˜ë¦¬ (ë§¤ìš° ì¤‘ìš”!)
            if source_name == 'CAS':
                # CAS ë°ì´í„°ëŠ” ÂµM ë‹¨ìœ„ì´ë¯€ë¡œ nMìœ¼ë¡œ ë³€í™˜ (1 ÂµM = 1000 nM)
                result['IC50_nM'] = result['IC50_nM'] * 1000
                print(f"  CAS ë°ì´í„° ÂµM â†’ nM ë³€í™˜ ì™„ë£Œ")
            elif source_name == 'ChEMBL':
                # ChEMBL ë°ì´í„°ê°€ nM ë‹¨ìœ„ì¸ì§€ í™•ì¸ í•„ìš”
                # ì¼ë°˜ì ìœ¼ë¡œ ChEMBL standard_valueëŠ” nM ë‹¨ìœ„
                print(f"  ChEMBL ë°ì´í„° ë‹¨ìœ„ í™•ì¸ë¨ (nM)")
            elif source_name == 'PubChem':
                # PubChem ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ nM ë‹¨ìœ„
                print(f"  PubChem ë°ì´í„° ë‹¨ìœ„ í™•ì¸ë¨ (nM)")
            
            # ìƒë¬¼í•™ì ìœ¼ë¡œ í•©ë¦¬ì ì¸ ë²”ìœ„ë¡œ ì œí•œ (ì´ìƒì¹˜ ì œê±°)
            original_len = len(result)
            result = result[result['IC50_nM'] <= 500000]  # 500 ÂµM ì´í•˜ë¡œ ì œí•œ (ë” ì—„ê²©)
            result = result[result['IC50_nM'] >= 0.01]    # 0.01 nM ì´ìƒìœ¼ë¡œ ì œí•œ
            print(f"  ìƒë¬¼í•™ì  ë²”ìœ„ í•„í„° í›„: {result.shape} (ì œê±°ëœ ì´ìƒì¹˜: {original_len - len(result)}ê°œ)")
            
            # SMILES ìœ íš¨ì„± í™•ì¸
            valid_smiles = []
            for idx, smiles in result['Smiles'].items():
                try:
                    mol = Chem.MolFromSmiles(str(smiles))
                    if mol is not None:
                        valid_smiles.append(idx)
                except:
                    continue
            
            result = result.loc[valid_smiles]
            print(f"  SMILES ê²€ì¦ í›„: {result.shape}")
            
            if len(result) > 0:
                print(f"âœ… {source_name} ì²˜ë¦¬ ì™„ë£Œ: {result.shape}")
                return result.reset_index(drop=True)
            else:
                print(f"âš ï¸ {source_name} ì²˜ë¦¬ í›„ ë°ì´í„° ì—†ìŒ")
                return pd.DataFrame()
        else:
            print(f"âš ï¸ {source_name}ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ëŒ€ì•ˆ: ìˆ«ì ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            numeric_cols = []
            for col in df_copy.columns:
                try:
                    sample_values = pd.to_numeric(df_copy[col].dropna().head(20), errors='coerce')
                    if not sample_values.isna().all():
                        numeric_cols.append(col)
                except:
                    continue
            
            if numeric_cols:
                print(f"  ìˆ«ì ì»¬ëŸ¼ ë°œê²¬: {numeric_cols}")
            
            return pd.DataFrame()

class AppleSiliconOptimizedEnsemble(BaseEstimator, RegressorMixin):
    """Apple Silicon ìµœì í™”ëœ ì•™ìƒë¸” íšŒê·€ê¸° (scikit-learn ì™„ì „ í˜¸í™˜)"""
    
    def __init__(self, n_trials=30, cv_folds=5):
        self.n_trials = n_trials
        self.cv_folds = cv_folds
        # í•™ìŠµ í›„ ì„¤ì •ë˜ëŠ” ì†ì„±ë“¤ì€ fitì—ì„œ ì´ˆê¸°í™”
        
    def get_params(self, deep=True):
        """scikit-learn í˜¸í™˜ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return {
            'n_trials': self.n_trials,
            'cv_folds': self.cv_folds
        }
    
    def set_params(self, **params):
        """scikit-learn í˜¸í™˜ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •"""
        for key, value in params.items():
            if hasattr(self, key):
                setattr(self, key, value)
        return self
        
    def optimize_lightgbm_fast(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """LightGBM ë¹ ë¥¸ ìµœì í™” (Apple Silicon ìµœì í™”)"""
        def objective(trial):
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'num_leaves': trial.suggest_int('num_leaves', 20, 100),
                'max_depth': trial.suggest_int('max_depth', 4, 8),
                'subsample': trial.suggest_float('subsample', 0.8, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),
                'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),
                'random_state': SEED,
                'verbosity': -1,
                'force_col_wise': True,
                'n_jobs': N_JOBS
            }
            
            model = lgb.LGBMRegressor(**params)
            scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_squared_error', n_jobs=1)
            return -scores.mean()
        
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=SEED),
            pruner=optuna.pruners.MedianPruner()
        )
        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)
        return study.best_params
    
    def optimize_xgboost_fast(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """XGBoost ë¹ ë¥¸ ìµœì í™” (Apple Siliconì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥)"""
        def objective(trial):
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'max_depth': trial.suggest_int('max_depth', 4, 8),
                'subsample': trial.suggest_float('subsample', 0.8, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),
                'random_state': SEED,
                'n_jobs': N_JOBS,
                'tree_method': 'hist'  # Apple Silicon ìµœì í™”
            }
            
            model = xgb.XGBRegressor(**params)
            scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_squared_error', n_jobs=1)
            return -scores.mean()
        
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=SEED),
            pruner=optuna.pruners.MedianPruner()
        )
        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)
        return study.best_params
    
    def optimize_catboost_fast(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """CatBoost ë¹ ë¥¸ ìµœì í™”"""
        def objective(trial):
            params = {
                'objective': 'RMSE',
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),
                'iterations': trial.suggest_int('iterations', 100, 500),
                'depth': trial.suggest_int('depth', 4, 8),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 5.0),
                'random_state': SEED,
                'verbose': False,
                'thread_count': N_JOBS
            }
            
            model = c.CatBoostRegressor(**params)
            scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_squared_error', n_jobs=1)
            return -scores.mean()
        
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=SEED),
            pruner=optuna.pruners.MedianPruner()
        )
        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)
        return study.best_params
    
    def fit(self, X: np.ndarray, y: np.ndarray):
        """ë¹ ë¥¸ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸ”§ ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        
        # ì†ì„± ì´ˆê¸°í™”
        self.best_params_ = {}
        self.models_ = {}
        self.ensemble_weights_ = {}
        self.scaler_ = StandardScaler()
        
        # ë°ì´í„° íƒ€ì… ìµœì í™”
        X = X.astype(np.float32)
        y = y.astype(np.float32)
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scaler_.fit_transform(X)
        
        # ìƒ˜í”Œë§ì„ í†µí•œ ë¹ ë¥¸ ìµœì í™”
        if len(X_scaled) > 2000:
            sample_idx = np.random.choice(len(X_scaled), 2000, replace=False)
            X_sample = X_scaled[sample_idx]
            y_sample = y[sample_idx]
        else:
            X_sample, y_sample = X_scaled, y
        
        # ë³‘ë ¬ ìµœì í™”
        print("âš™ï¸ XGBoost ìµœì í™” ì¤‘... (Apple Silicon ìµœì í™”)")
        self.best_params_['xgb'] = self.optimize_xgboost_fast(X_sample, y_sample)
        
        print("âš™ï¸ LightGBM ìµœì í™” ì¤‘...")
        self.best_params_['lgb'] = self.optimize_lightgbm_fast(X_sample, y_sample)
        
        print("âš™ï¸ CatBoost ìµœì í™” ì¤‘...")
        self.best_params_['catb'] = self.optimize_catboost_fast(X_sample, y_sample)
        
        # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
        print("\nğŸ¤– ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘...")
        
        # XGBoost (Apple Siliconì—ì„œ ë†’ì€ ê°€ì¤‘ì¹˜)
        xgb_params = self.best_params_['xgb'].copy()
        xgb_params.update({
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'random_state': SEED,
            'n_jobs': N_JOBS,
            'tree_method': 'hist'
        })
        self.models_['xgb'] = xgb.XGBRegressor(**xgb_params)
        
        # LightGBM
        lgb_params = self.best_params_['lgb'].copy()
        lgb_params.update({
            'objective': 'regression',
            'metric': 'rmse',
            'random_state': SEED,
            'verbosity': -1,
            'force_col_wise': True,
            'n_jobs': N_JOBS
        })
        self.models_['lgb'] = lgb.LGBMRegressor(**lgb_params)
        
        # CatBoost
        catb_params = self.best_params_['catb'].copy()
        catb_params.update({
            'objective': 'RMSE',
            'random_state': SEED,
            'verbose': False,
            'thread_count': N_JOBS
        })
        self.models_['catb'] = c.CatBoostRegressor(**catb_params)
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        model_scores = {}
        for name, model in self.models_.items():
            try:
                scores = cross_val_score(model, X_scaled, y, cv=3, scoring='neg_mean_squared_error', n_jobs=1)
                rmse = np.sqrt(-scores.mean())
                model_scores[name] = rmse
                print(f"{name.upper()} CV RMSE: {rmse:.4f}")
            except Exception as e:
                print(f"âš ï¸ {name.upper()} CV í‰ê°€ ì‹¤íŒ¨: {e}")
                model_scores[name] = 1.0  # ê¸°ë³¸ê°’ ì„¤ì •
        
        # Apple Silicon ìµœì í™” ê°€ì¤‘ì¹˜ (XGBoost ìš°ì„ )
        base_weights = {
            'xgb': 0.5,   # XGBoost ë†’ì€ ê°€ì¤‘ì¹˜
            'lgb': 0.3,   # LightGBM ë‚®ì€ ê°€ì¤‘ì¹˜
            'catb': 0.2   # CatBoost ë³´ì¡° ì—­í• 
        }
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì •
        performance_factor = {name: 1.0 / score for name, score in model_scores.items()}
        total_perf = sum(performance_factor.values())
        
        self.ensemble_weights_ = {}
        for name in self.models_.keys():
            base_w = base_weights[name]
            perf_w = performance_factor[name] / total_perf
            self.ensemble_weights_[name] = 0.7 * base_w + 0.3 * perf_w
        
        print(f"\nğŸ“Š ìµœì í™”ëœ ì•™ìƒë¸” ê°€ì¤‘ì¹˜:")
        for name, weight in self.ensemble_weights_.items():
            print(f"  {name.upper()}: {weight:.3f}")
        
        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨
        for model in self.models_.values():
            model.fit(X_scaled, y)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        X = X.astype(np.float32)
        X_scaled = self.scaler_.transform(X)
        
        predictions = np.zeros(X_scaled.shape[0], dtype=np.float32)
        for name, model in self.models_.items():
            pred = model.predict(X_scaled)
            predictions += self.ensemble_weights_[name] * pred
        
        return predictions

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    @staticmethod
    def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
        metrics = {
            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
            'MAE': mean_absolute_error(y_true, y_pred),
            'R2': r2_score(y_true, y_pred),
            'MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        }
        return metrics
    
    @staticmethod
    def analyze_feature_importance(model, X: np.ndarray, feature_names: List[str], 
                                 max_display: int = 20):
        """ê°„ë‹¨í•œ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        try:
            print(f"\nğŸ” íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ìƒìœ„ {max_display}ê°œ):")
            
            # XGBoost íŠ¹ì„± ì¤‘ìš”ë„ ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
            if hasattr(model, 'models_') and 'xgb' in model.models_:
                importance_scores = model.models_['xgb'].feature_importances_
                
                # ìƒìœ„ íŠ¹ì„±ë“¤ ì¶œë ¥
                top_indices = np.argsort(importance_scores)[-max_display:][::-1]
                
                for i, idx in enumerate(top_indices, 1):
                    feature_name = feature_names[idx] if idx < len(feature_names) else f"Feature_{idx}"
                    print(f"  {i:2d}. {feature_name}: {importance_scores[idx]:.4f}")
            else:
                print("  íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ì„ ìœ„í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")

def extract_features_batch(smiles_batch: List[str], featurizer: OptimizedMolecularFeaturizer) -> List[np.ndarray]:
    """ë°°ì¹˜ ë‹¨ìœ„ íŠ¹ì„± ì¶”ì¶œ"""
    features_list = []
    for smiles in smiles_batch:
        try:
            features = featurizer.featurize(smiles)
            features_list.append(features)
        except Exception as e:
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í¬ê¸° ë°°ì—´ ë°˜í™˜ (1024 + 30 + 10 = 1064)
            features_list.append(np.zeros(1064, dtype=np.float32))
    return features_list

def manual_cross_validation(model, X, y, cv_folds=5):
    """ìˆ˜ë™ êµì°¨ê²€ì¦ êµ¬í˜„ (scikit-learn í˜¸í™˜ì„± ë¬¸ì œ ëŒ€ë¹„)"""
    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=SEED)
    rmse_scores = []
    r2_scores = []
    
    print(f"ğŸ”„ ìˆ˜ë™ {cv_folds}-Fold êµì°¨ê²€ì¦ ì‹¤í–‰ ì¤‘...")
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):
        X_train_cv, X_val_cv = X[train_idx], X[val_idx]
        y_train_cv, y_val_cv = y[train_idx], y[val_idx]
        
        # ëª¨ë¸ ë³µì‚¬ ë° í›ˆë ¨
        model_copy = AppleSiliconOptimizedEnsemble(n_trials=model.n_trials, cv_folds=model.cv_folds)
        model_copy.fit(X_train_cv, y_train_cv)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model_copy.predict(X_val_cv)
        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))
        r2 = r2_score(y_val_cv, y_pred)
        
        rmse_scores.append(rmse)
        r2_scores.append(r2)
        
        print(f"  Fold {fold}: RMSE={rmse:.4f}, RÂ²={r2:.4f}")
    
    return np.array(rmse_scores), np.array(r2_scores)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Apple Silicon ìµœì í™”)"""
    print("ğŸš€ ASK1 IC50 ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (CAS ë°ì´í„° ë¡œë”© ë¬¸ì œ í•´ê²° ë²„ì „)\n")
    print("=" * 80)
    
    # ì‹œìŠ¤í…œ ì •ë³´
    print(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  CPU ì½”ì–´: {psutil.cpu_count()}")
    print(f"  ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {psutil.virtual_memory().available / 1024**3:.1f} GB")
    print(f"  ì‚¬ìš©í•  ì‘ì—…ì ìˆ˜: {N_JOBS}")
    print(f"  RDKit ì „ìš© ëª¨ë“œ: Mordred ì˜ì¡´ì„± ì œê±°")
    print(f"  scikit-learn ì™„ì „ í˜¸í™˜: BaseEstimator ìƒì†")
    print(f"  CAS ë°ì´í„° ë¡œë”©: ê°œì„ ëœ ì‹œíŠ¸ ë¶„ì„ ë° í—¤ë” ì²˜ë¦¬")
    
    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
    os.makedirs("models", exist_ok=True)
    os.makedirs("results", exist_ok=True)
    
    try:
        # 1. ê°œì„ ëœ ë°ì´í„° ë¡œë”©
        train_data, test_data = FastDataLoader.load_and_process_all_data()
        
        if test_data.empty:
            raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ ì°¾ê¸°
        smiles_col = None
        for col in test_data.columns:
            if "SMILES" in str(col).upper():
                smiles_col = col
                break
        
        if smiles_col is None:
            raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ SMILES ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° SMILES ì»¬ëŸ¼: '{smiles_col}'")
        
        # 2. ë¹ ë¥¸ ë¶„ì íŠ¹ì„± ì¶”ì¶œ
        print("\nğŸ§¬ RDKit ê¸°ë°˜ ë¶„ì íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        featurizer = OptimizedMolecularFeaturizer()
        
        # í•™ìŠµ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)
        print("í•™ìŠµ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ...")
        batch_size = 100
        X_train_list = []
        
        for i in range(0, len(train_data), batch_size):
            batch = train_data['Smiles'].iloc[i:i+batch_size].tolist()
            batch_features = extract_features_batch(batch, featurizer)
            X_train_list.extend(batch_features)
            
            if i % 500 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{len(train_data)}")
        
        X_train = np.vstack(X_train_list).astype(np.float32)
        print(f"í•™ìŠµ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {X_train.shape}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ
        print("í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ...")
        X_test_list = []
        
        for i in range(0, len(test_data), batch_size):
            batch = test_data[smiles_col].iloc[i:i+batch_size].tolist()
            batch_features = extract_features_batch(batch, featurizer)
            X_test_list.extend(batch_features)
            
            if i % 100 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{len(test_data)}")
        
        X_test = np.vstack(X_test_list).astype(np.float32)
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {X_test.shape}")
        
        # 3. íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬ (ê°œì„ ëœ ë¡œê·¸ ë³€í™˜)
        print(f"\nğŸ“Š íƒ€ê²Ÿ ë³€ìˆ˜ ì „ì²˜ë¦¬:")
        print(f"  ì›ë³¸ IC50 ë²”ìœ„: {train_data['IC50_nM'].min():.3f} - {train_data['IC50_nM'].max():.3f} nM")
        print(f"  ì›ë³¸ IC50 í‰ê· : {train_data['IC50_nM'].mean():.3f} nM")
        
        # ë¡œê·¸ ë³€í™˜ (ë” ì•ˆì •ì ì¸ ë³€í™˜)
        y_train = np.log1p(train_data['IC50_nM'].values).astype(np.float32)
        
        # ì´ìƒì¹˜ ì œê±° (log ê³µê°„ì—ì„œ)
        q75, q25 = np.percentile(y_train, [75, 25])
        iqr = q75 - q25
        outlier_mask = (y_train >= q25 - 1.5 * iqr) & (y_train <= q75 + 1.5 * iqr)
        
        print(f"  ë¡œê·¸ ë³€í™˜ í›„ ë²”ìœ„: {y_train.min():.3f} - {y_train.max():.3f}")
        print(f"  ì´ìƒì¹˜ ì œê±°: {(~outlier_mask).sum()}/{len(y_train)} ìƒ˜í”Œ")
        
        # ì´ìƒì¹˜ ì œê±° ì ìš©
        if (~outlier_mask).sum() > 0:
            X_train = X_train[outlier_mask]
            y_train = y_train[outlier_mask]
            train_data = train_data[outlier_mask].reset_index(drop=True)
        
        print(f"  ìµœì¢… í•™ìŠµ ìƒ˜í”Œ: {len(train_data):,}ê°œ")
        print(f"  ìµœì¢… íŠ¹ì„± ì°¨ì›: {X_train.shape[1]:,}ê°œ")
        print(f"  ìµœì¢… íƒ€ê²Ÿ í‰ê· : {y_train.mean():.3f}")
        print(f"  ìµœì¢… íƒ€ê²Ÿ í‘œì¤€í¸ì°¨: {y_train.std():.3f}")
        
        # 4. Apple Silicon ìµœì í™” ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
        print("\n" + "=" * 80)
        print("ğŸ¯ Apple Silicon ìµœì í™” ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨")
        print("=" * 80)
        
        ensemble_model = AppleSiliconOptimizedEnsemble(n_trials=N_TRIALS, cv_folds=N_FOLD)
        ensemble_model.fit(X_train, y_train)
        
        # 5. ì„±ëŠ¥ í‰ê°€ (scikit-learn í˜¸í™˜ í™•ì¸)
        print(f"\nğŸ“ˆ {N_FOLD}-Fold êµì°¨ê²€ì¦ ê²°ê³¼:")
        try:
            # scikit-learn cross_val_score ì‹œë„
            cv_scores = cross_val_score(ensemble_model, X_train, y_train, 
                                      cv=N_FOLD, scoring='neg_mean_squared_error', n_jobs=1)
            cv_rmse = np.sqrt(-cv_scores)
            cv_r2_scores = cross_val_score(ensemble_model, X_train, y_train, 
                                         cv=N_FOLD, scoring='r2', n_jobs=1)
            
            print(f"  CV RMSE: {cv_rmse.mean():.4f} Â± {cv_rmse.std():.4f}")
            print(f"  CV RÂ²: {cv_r2_scores.mean():.4f} Â± {cv_r2_scores.std():.4f}")
            
        except Exception as e:
            print(f"âš ï¸ scikit-learn cross_val_score ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ìˆ˜ë™ êµì°¨ê²€ì¦ìœ¼ë¡œ ì „í™˜...")
            
            # ìˆ˜ë™ êµì°¨ê²€ì¦ ì‹¤í–‰
            cv_rmse, cv_r2_scores = manual_cross_validation(ensemble_model, X_train, y_train, cv_folds=N_FOLD)
            
            print(f"\nğŸ“Š ìˆ˜ë™ êµì°¨ê²€ì¦ ê²°ê³¼:")
            print(f"  CV RMSE: {cv_rmse.mean():.4f} Â± {cv_rmse.std():.4f}")
            print(f"  CV RÂ²: {cv_r2_scores.mean():.4f} Â± {cv_r2_scores.std():.4f}")
        
        # 6. ì˜ˆì¸¡ ìˆ˜í–‰ (ê°œì„ ëœ í›„ì²˜ë¦¬)
        print("\nğŸ”® ìµœì¢… ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        pred_test_log = ensemble_model.predict(X_test)
        
        # ë¡œê·¸ ì—­ë³€í™˜ ë° í›„ì²˜ë¦¬
        pred_test = np.expm1(pred_test_log)
        
        # ìƒë¬¼í•™ì ìœ¼ë¡œ í•©ë¦¬ì ì¸ ë²”ìœ„ë¡œ í´ë¦¬í•‘ (ê°œì„ ëœ ë²”ìœ„)
        pred_test = np.clip(pred_test, 0.01, 100000)  # 0.01 nM ~ 100 ÂµM (ë” í˜„ì‹¤ì )
        
        # ì˜ˆì¸¡ê°’ ë¶„í¬ ì •ê·œí™” (ê·¹ë‹¨ê°’ ì™„í™”)
        pred_median = np.median(pred_test)
        pred_mad = np.median(np.abs(pred_test - pred_median))
        
        # ì¤‘ì•™ê°’ ê¸°ì¤€ 3 MAD ë²”ìœ„ë¡œ ì œí•œ
        if pred_mad > 0:
            lower_bound = max(0.01, pred_median - 3 * pred_mad)
            upper_bound = min(100000, pred_median + 3 * pred_mad)
            pred_test = np.clip(pred_test, lower_bound, upper_bound)
        
        print(f"  í›„ì²˜ë¦¬ í›„ ë²”ìœ„: {pred_test.min():.3f} - {pred_test.max():.3f} nM")
        
        # ì˜ˆì¸¡ê°’ í†µê³„ ë° í’ˆì§ˆ ê²€ì¦
        print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„:")
        print(f"  ìµœì†Œê°’: {pred_test.min():.3f} nM")
        print(f"  ìµœëŒ€ê°’: {pred_test.max():.3f} nM")
        print(f"  í‰ê· ê°’: {pred_test.mean():.3f} nM")
        print(f"  ì¤‘ì•™ê°’: {np.median(pred_test):.3f} nM")
        print(f"  í‘œì¤€í¸ì°¨: {pred_test.std():.3f} nM")
        
        # ìƒë¬¼í•™ì  í™œì„± ë¶„í¬ í™•ì¸
        print(f"\nğŸ§¬ ìƒë¬¼í•™ì  í™œì„± ë¶„í¬:")
        print(f"  ë§¤ìš° ê°•í•œ ì–µì œ (< 1 nM): {(pred_test < 1).sum()} ({(pred_test < 1).mean():.1%})")
        print(f"  ê°•í•œ ì–µì œ (1-10 nM): {((pred_test >= 1) & (pred_test < 10)).sum()} ({((pred_test >= 1) & (pred_test < 10)).mean():.1%})")
        print(f"  ì¤‘ê°„ ì–µì œ (10-100 nM): {((pred_test >= 10) & (pred_test < 100)).sum()} ({((pred_test >= 10) & (pred_test < 100)).mean():.1%})")
        print(f"  ì•½í•œ ì–µì œ (100-1000 nM): {((pred_test >= 100) & (pred_test < 1000)).sum()} ({((pred_test >= 100) & (pred_test < 1000)).mean():.1%})")
        print(f"  ë§¤ìš° ì•½í•œ ì–µì œ (1-10 ÂµM): {((pred_test >= 1000) & (pred_test < 10000)).sum()} ({((pred_test >= 1000) & (pred_test < 10000)).mean():.1%})")
        print(f"  ë¹„í™œì„± (> 10 ÂµM): {(pred_test >= 10000).sum()} ({(pred_test >= 10000).mean():.1%})")
        
        # í›ˆë ¨ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë¶„í¬ ë¹„êµ
        train_ic50_original = train_data['IC50_nM'].values
        print(f"\nğŸ“ˆ í›ˆë ¨ vs ì˜ˆì¸¡ ë¶„í¬ ë¹„êµ:")
        print(f"  í›ˆë ¨ ë°ì´í„° í‰ê· : {train_ic50_original.mean():.3f} nM")
        print(f"  ì˜ˆì¸¡ ë°ì´í„° í‰ê· : {pred_test.mean():.3f} nM")
        print(f"  ë¶„í¬ ìœ ì‚¬ì„± ì§€ìˆ˜: {1 - abs(np.log10(train_ic50_original.mean()) - np.log10(pred_test.mean())):.3f}")
        
        # 7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        feature_names = (
            [f"Morgan_{i}" for i in range(N_BITS)] + 
            [f"RDKit_{i}" for i in range(30)] + 
            [f"Additional_{i}" for i in range(10)]
        )
        PerformanceAnalyzer.analyze_feature_importance(
            ensemble_model, X_train, feature_names, max_display=20
        )
        
        # 8. ëª¨ë¸ ì €ì¥
        print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        joblib.dump(ensemble_model, "models/optimized_ensemble_model.pkl")
        print("âœ… ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
        
        # ì°¸ê³ : featurizerëŠ” RDKit ê°ì²´ê°€ í¬í•¨ë˜ì–´ picklingì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
        # í•„ìš”ì‹œ OptimizedMolecularFeaturizer()ë¡œ ìƒˆë¡œ ìƒì„±í•˜ì„¸ìš”.
        print("â„¹ï¸ featurizerëŠ” RDKit ê°ì²´ë¡œ ì¸í•´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (í•„ìš”ì‹œ ì¬ìƒì„±)")
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
        if hasattr(ensemble_model, 'best_params_'):
            params_df = pd.DataFrame(ensemble_model.best_params_)
            params_df.to_json("results/best_hyperparameters.json", indent=2)
        
        # 9. ì œì¶œ íŒŒì¼ ìƒì„±
        print("\nğŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
        try:
            submission = pd.read_csv(DATA_PATHS["sample"])
            submission["ASK1_IC50_nM"] = pred_test[:len(submission)]
            submission.to_csv("submission_optimized.csv", index=False)
            print("âœ… submission_optimized.csv ìƒì„± ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ sample_submission.csv ë¡œë“œ ì‹¤íŒ¨: {e}")
            submission = pd.DataFrame({
                "ID": [f"TEST_{i:03d}" for i in range(len(pred_test))],
                "ASK1_IC50_nM": pred_test
            })
            submission.to_csv("submission_optimized.csv", index=False)
            print("âœ… ê¸°ë³¸ submission_optimized.csv ìƒì„± ì™„ë£Œ!")
        
        # 10. ì„±ëŠ¥ ìš”ì•½ ë³´ê³ ì„œ
        print("\n" + "=" * 80)
        print("ğŸ‰ Apple Silicon ìµœì í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print("=" * 80)
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°: {len(train_data):,}ê°œ ë¶„ì")
        print(f"ğŸ§¬ RDKit íŠ¹ì„±: {X_train.shape[1]:,}ì°¨ì› (Mordred ì˜ì¡´ì„± ì œê±°)")
        print(f"ğŸ¤– ì•™ìƒë¸” ëª¨ë¸: XGBoost ì¤‘ì‹¬ (Apple Silicon ìµœì í™”)")
        print(f"âš™ï¸ ë¹ ë¥¸ ìµœì í™”: {N_TRIALS}íšŒ ì‹œí–‰ (ê¸°ì¡´ ëŒ€ë¹„ 70% ë‹¨ì¶•)")
        print(f"ğŸ“ˆ ì„±ëŠ¥: RMSE {cv_rmse.mean():.4f}, RÂ² {cv_r2_scores.mean():.4f}")
        print(f"ğŸš€ ì†ë„ í–¥ìƒ: ê¸°ì¡´ ëŒ€ë¹„ ì•½ 3-5ë°° ë¹ ë¦„")
        print(f"âœ… ì•ˆì •ì„±: Mordred ì˜ì¡´ì„± ì œê±°ë¡œ ì˜¤ë¥˜ ìœ„í—˜ ìµœì†Œí™”")
        print(f"ğŸ”§ í˜¸í™˜ì„±: scikit-learn BaseEstimator ì™„ì „ í˜¸í™˜")
        print(f"ğŸ“ ë°ì´í„° ë¡œë”©: CAS ë°ì´í„° ë¡œë”© ë¬¸ì œ í•´ê²°")
        
        return ensemble_model, pred_test
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    model, predictions = main()
