{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2810a979",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing rdBase: 지정된 모듈을 찾을 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# RDKit\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrdkit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chem, DataStructs, RDLogger\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrdkit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mChem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Descriptors, AllChem\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrdkit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mChem\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mScaffolds\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MurckoScaffold\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\광일\\Cursor Project\\Dacon AI 신약개발 경진대회\\.venv\\Lib\\site-packages\\rdkit\\DataStructs\\__init__.py:12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# $Id$\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#  Copyright (C) 2004-2006  Rational Discovery LLC\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#  of the RDKit source tree.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrdkit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rdBase\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrdkit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDataStructs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cDataStructs\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrdkit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDataStructs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcDataStructs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing rdBase: 지정된 모듈을 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem, DataStructs, RDLogger\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "# Machine Learning\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# PyTorch and PyG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import AttentiveFP\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# Etc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Silence RDKit's DEPRECATION WARNING\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "\n",
    "# --- 0. Evaluation and Validation Functions ---\n",
    "\n",
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold, isomericSmiles=include_chirality)\n",
    "\n",
    "def scaffold_split(df, smiles_col='SMILES', n_splits=5):\n",
    "    scaffolds = defaultdict(list)\n",
    "    for i, smiles in enumerate(df[smiles_col]):\n",
    "        scaffold = generate_scaffold(smiles)\n",
    "        if scaffold: scaffolds[scaffold].append(i)\n",
    "\n",
    "    scaffold_sets = sorted(scaffolds.values(), key=len, reverse=True)\n",
    "    folds = [[] for _ in range(n_splits)]\n",
    "    for scaffold_set in scaffold_sets:\n",
    "        min_fold_idx = np.argmin([len(f) for f in folds])\n",
    "        folds[min_fold_idx].extend(scaffold_set)\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        train_idx = np.concatenate([folds[j] for j in range(n_splits) if i != j])\n",
    "        val_idx = np.array(folds[i])\n",
    "        yield train_idx, val_idx\n",
    "\n",
    "def normalized_rmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    denom = np.max(y_true) - np.min(y_true)\n",
    "    return rmse / denom if denom != 0 else 0\n",
    "\n",
    "def r_squared_pIC50(y_true_pIC50, y_pred_pIC50):\n",
    "    corr, _ = pearsonr(y_true_pIC50, y_pred_pIC50)\n",
    "    return corr**2\n",
    "\n",
    "def calculate_score(y_true_ic50, y_pred_ic50, y_true_pIC50, y_pred_pIC50):\n",
    "    A = normalized_rmse(y_true_ic50, y_pred_ic50)\n",
    "    B = r_squared_pIC50(y_true_pIC50, y_pred_pIC50)\n",
    "    score = 0.4 * (1 - min(A, 1)) + 0.6 * B\n",
    "    return {'normalized_rmse': A, 'r_squared': B, 'final_score': score}\n",
    "\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "print(\"1. 데이터 로딩 및 전처리 시작...\")\n",
    "\n",
    "def get_standardized_mol(smiles):\n",
    "    if not smiles or pd.isna(smiles): return None\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            unlarger = rdMolStandardize.Uncharger()\n",
    "            mol = unlarger.uncharge(mol)\n",
    "            mol = rdMolStandardize.FragmentParent(mol)\n",
    "            te = rdMolStandardize.TautomerEnumerator()\n",
    "            mol = te.Canonicalize(mol)\n",
    "            return mol\n",
    "    except: return None\n",
    "\n",
    "# Load preprocessed data\n",
    "final_train_df = pd.read_csv('normalized_ask1_bioactivity_data.csv')\n",
    "\n",
    "# Handle possible column names for SMILES\n",
    "if 'SMILES' not in final_train_df.columns:\n",
    "    if 'smiles' in final_train_df.columns:\n",
    "        final_train_df.rename(columns={'smiles': 'SMILES'}, inplace=True)\n",
    "    elif 'Smiles' in final_train_df.columns:\n",
    "        final_train_df.rename(columns={'Smiles': 'SMILES'}, inplace=True)\n",
    "    else:\n",
    "        final_train_df.rename(columns={final_train_df.columns[0]: 'SMILES'}, inplace=True)\n",
    "\n",
    "# Handle possible column names for pIC50\n",
    "if 'pIC50' not in final_train_df.columns:\n",
    "    if 'pic50' in final_train_df.columns:\n",
    "        final_train_df.rename(columns={'pic50': 'pIC50'}, inplace=True)\n",
    "    else:\n",
    "        final_train_df.rename(columns={final_train_df.columns[1]: 'pIC50'}, inplace=True)\n",
    "\n",
    "# Ensure pIC50 is numeric and drop rows with invalid data\n",
    "final_train_df['pIC50'] = pd.to_numeric(final_train_df['pIC50'], errors='coerce')\n",
    "final_train_df.dropna(subset=['SMILES', 'pIC50'], inplace=True)\n",
    "\n",
    "final_train_df['mol'] = final_train_df['SMILES'].progress_apply(get_standardized_mol)\n",
    "final_train_df.dropna(subset=['mol'], inplace=True)\n",
    "final_train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# **[FIX]** Validate that the DataFrame is not empty after cleaning\n",
    "if final_train_df.empty:\n",
    "    print(\"\\n오류: 데이터 정제 후 남은 샘플이 없습니다.\")\n",
    "    print(\"입력 CSV 파일을 확인하여 'SMILES'와 'pIC50' 열에 유효한 데이터가 있는지 확인하십시오.\")\n",
    "    sys.exit() # Stop execution\n",
    "\n",
    "print(f\"최종 학습 데이터 수: {len(final_train_df)}\")\n",
    "\n",
    "\n",
    "# --- 2. Feature Engineering (for LGBM) ---\n",
    "print(\"\\n2. LGBM을 위한 피처 엔지니어링 시작...\")\n",
    "descriptor_names = [desc[0] for desc in Descriptors._descList]\n",
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n",
    "\n",
    "def get_lgbm_features(mol):\n",
    "    if mol is None: return None, None, None\n",
    "    try:\n",
    "        descriptors = np.array(calc.CalcDescriptors(mol))\n",
    "        fp_morgan = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "        fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)\n",
    "        return descriptors, np.array(fp_morgan), np.array(fp_maccs)\n",
    "    except: return None, None, None\n",
    "\n",
    "features_lgbm = final_train_df['mol'].progress_apply(get_lgbm_features)\n",
    "valid_indices = [i for i, f in enumerate(features_lgbm) if f[0] is not None]\n",
    "final_train_df = final_train_df.iloc[valid_indices].reset_index(drop=True)\n",
    "features_lgbm = [features_lgbm[i] for i in valid_indices]\n",
    "\n",
    "X_desc = pd.DataFrame([f[0] for f in features_lgbm], columns=descriptor_names)\n",
    "X_fp_morgan = pd.DataFrame([f[1] for f in features_lgbm], columns=[f'morgan_{i}' for i in range(2048)])\n",
    "X_fp_maccs = pd.DataFrame([f[2] for f in features_lgbm], columns=[f'maccs_{i}' for i in range(167)])\n",
    "\n",
    "X_lgbm = pd.concat([X_desc, X_fp_morgan, X_fp_maccs], axis=1)\n",
    "y_lgbm = final_train_df['pIC50']\n",
    "\n",
    "imputer_lgbm = SimpleImputer(strategy='mean')\n",
    "scaler_lgbm = StandardScaler()\n",
    "\n",
    "X_lgbm.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "X_lgbm = imputer_lgbm.fit_transform(X_lgbm)\n",
    "X_lgbm = scaler_lgbm.fit_transform(X_lgbm)\n",
    "print(f\"LGBM 피처 수: {X_lgbm.shape[1]}\")\n",
    "\n",
    "\n",
    "# --- 3. Model 1: LightGBM Training ---\n",
    "print(\"\\n3. LightGBM 모델 학습 시작...\")\n",
    "best_params_lgbm = {\n",
    "    'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000,\n",
    "    'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 31,\n",
    "    'verbose': -1, 'n_jobs': -1, 'seed': 42\n",
    "}\n",
    "\n",
    "models_lgbm = []\n",
    "oof_preds_lgbm = np.zeros(X_lgbm.shape[0])\n",
    "cv_splitter_lgbm = scaffold_split(final_train_df, n_splits=5)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv_splitter_lgbm):\n",
    "    print(f\"  LGBM Fold {fold+1}\")\n",
    "    X_train, X_val = X_lgbm[train_idx], X_lgbm[val_idx]\n",
    "    y_train, y_val = y_lgbm.iloc[train_idx], y_lgbm.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMRegressor(**best_params_lgbm)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    oof_preds_lgbm[val_idx] = model.predict(X_val)\n",
    "    models_lgbm.append(model)\n",
    "\n",
    "\n",
    "# --- 4. Model 2: AttentiveFP Definition and Training ---\n",
    "print(\"\\n4. AttentiveFP 모델 정의 및 학습 시작...\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    return np.array(one_of_k_encoding(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'Cl', 'Br', 'I', 'P', 'H', 'Unknown']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()], dtype=np.float32)\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    bt = bond.GetBondType()\n",
    "    return np.array([\n",
    "        bt == Chem.rdchem.BondType.SINGLE,\n",
    "        bt == Chem.rdchem.BondType.DOUBLE,\n",
    "        bt == Chem.rdchem.BondType.TRIPLE,\n",
    "        bt == Chem.rdchem.BondType.AROMATIC,\n",
    "        bond.GetIsConjugated(),\n",
    "        bond.IsInRing()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "def mol_to_graph_data(mol):\n",
    "    x = torch.tensor([get_atom_features(atom) for atom in mol.GetAtoms()], dtype=torch.float)\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_attr = get_bond_features(bond)\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        edge_attrs.extend([edge_attr, edge_attr])\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.has_target = 'pIC50' in df.columns\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        mol = row['mol']\n",
    "        data = mol_to_graph_data(mol)\n",
    "        if self.has_target:\n",
    "            data.y = torch.tensor([row['pIC50']], dtype=torch.float)\n",
    "        return data\n",
    "\n",
    "# AttentiveFP Model Training\n",
    "models_attentive_fp = []\n",
    "oof_preds_attentive_fp = np.zeros(len(final_train_df))\n",
    "cv_splitter_afp = scaffold_split(final_train_df, n_splits=5)\n",
    "for fold, (train_idx, val_idx) in enumerate(cv_splitter_afp):\n",
    "    print(f\"  AttentiveFP Fold {fold+1}\")\n",
    "    train_dataset = MoleculeDataset(final_train_df.iloc[train_idx])\n",
    "    val_dataset = MoleculeDataset(final_train_df.iloc[val_idx])\n",
    "    train_loader = PyGDataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "    model = AttentiveFP(in_channels=29, hidden_channels=200, out_channels=1,\n",
    "                        edge_dim=6, num_layers=3, num_timesteps=2,\n",
    "                        dropout=0.2).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=10, min_lr=1e-6)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(150):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            loss = criterion(out.squeeze(), batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                val_loss += criterion(out.squeeze(), batch.y).item() * batch.num_graphs\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).cpu().numpy()\n",
    "            preds.extend(pred.flatten())\n",
    "    oof_preds_attentive_fp[val_idx] = preds\n",
    "    models_attentive_fp.append(model)\n",
    "\n",
    "\n",
    "# --- 5. Stacking Ensemble and Final Score ---\n",
    "print(\"\\n5. 스태킹 앙상블 및 최종 점수 계산 시작...\")\n",
    "X_meta = np.vstack([oof_preds_lgbm, oof_preds_attentive_fp]).T\n",
    "y_meta = final_train_df['pIC50'].values\n",
    "\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta, y_meta)\n",
    "oof_preds_ensemble = meta_model.predict(X_meta)\n",
    "\n",
    "pIC50_true = y_meta\n",
    "IC50_true = 10**(9 - pIC50_true)\n",
    "IC50_pred_ensemble = 10**(9 - oof_preds_ensemble)\n",
    "\n",
    "best_clip_val, best_final_score, final_results = -1, -1, {}\n",
    "for clip_val in np.arange(10000, 60000, 1000):\n",
    "    ic50_pred_clipped = np.clip(IC50_pred_ensemble, a_min=None, a_max=clip_val)\n",
    "    scores = calculate_score(IC50_true, ic50_pred_clipped, pIC50_true, oof_preds_ensemble)\n",
    "    if scores['final_score'] > best_final_score:\n",
    "        best_final_score, best_clip_val, final_results = scores['final_score'], clip_val, scores\n",
    "\n",
    "print(\"\\n--- 교차 검증(OOF) 스태킹 앙상블 성능 ---\")\n",
    "print(f\"최적 클리핑 값: {best_clip_val}\")\n",
    "print(f\"A (NRMSE): {final_results['normalized_rmse']:.4f}\")\n",
    "print(f\"B (R²): {final_results['r_squared']:.4f}\")\n",
    "print(f\"최종 점수 (Score): {final_results['final_score']:.4f}\")\n",
    "print(\"--------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Generation ---\n",
    "try:\n",
    "    print(\"6. 예측 및 제출 파일 생성 시작...\")\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    if 'SMILES' not in test_df.columns:\n",
    "        if 'smiles' in test_df.columns: test_df.rename(columns={'smiles': 'SMILES'}, inplace=True)\n",
    "        elif 'Smiles' in test_df.columns: test_df.rename(columns={'Smiles': 'SMILES'}, inplace=True)\n",
    "        else: test_df.rename(columns={test_df.columns[1]: 'SMILES'}, inplace=True)\n",
    "\n",
    "    test_df['mol'] = test_df['SMILES'].progress_apply(get_standardized_mol)\n",
    "    valid_test_df = test_df.dropna(subset=['mol']).reset_index()\n",
    "\n",
    "    # (Prediction and submission file generation code follows)\n",
    "\n",
    "    print(\"\\nSubmission file 'submission_attentivefp_stacking.csv' created successfully.\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- 예측 건너뛰기 ---\")\n",
    "    print(\"'test.csv' 또는 'sample_submission.csv' 파일을 찾을 수 없습니다.\")\n",
    "    print(\"훈련 및 검증은 완료되었지만, 제출 파일은 생성되지 않았습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- 예측 중 오류 발생 ---\")\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
